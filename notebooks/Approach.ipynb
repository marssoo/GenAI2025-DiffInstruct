{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "This notebook will present how we conducted our experiments, and will evaluate our various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Outline of the approach\n",
    "We want to apply [DiffInstruct](https://arxiv.org/abs/2305.18455) to finetune a generator (obtained with adversarial training) with a better diffusion model (DM). The main goal of DiffInstruct is to distill the knowledge of a DM (typically very powerful but long to sample from) into a weaker model from which sampling is fast. DiffInstruct (DI), as presented in the original paper, can be applied to single-step DMs as well as generators as long as the generated samples are differentiable with respect to the generator's parameters. In the associated [Github](https://github.com/pkulwj1994/diff_instruct/tree/main), the authors focus on single step DMs. We will here focus on a generated trained with an adversarial procedure. The code that implements the DI method (in `DiffInstruct.py`) is heavily inspired from the previous Github repository. \n",
    "\n",
    "We will here provide an overview of:\n",
    "- The training of our teacher DM.\n",
    "- The training of our student generator.\n",
    "- The distillation of the teacher's knowledge to the student by DI.\n",
    "- Evaluate all these models to see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trainings\n",
    "Here we detail the obtention of the DM and the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
